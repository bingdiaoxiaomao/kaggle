{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import time\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydotplus\n",
    "plt.style.use('classic')\n",
    "%matplotlib inline\n",
    "\n",
    "from smart_open import smart_open\n",
    "from string import punctuation\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,Input,LSTM,Embedding,Dropout,Activation\n",
    "from keras.layers.merge import concatenate,dot\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置数据路径及模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = '../inputs/'\n",
    "\n",
    "# glove词向量\n",
    "EMBEDDING_FILE = BASE_DIR + 'glove.6B/glove.6B.50d.txt'\n",
    "# 训练数据与测试数据\n",
    "TRAIN_DATA_FILE  = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 50\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 50\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lstm = np.random.randint(175,275)\n",
    "num_dense = np.random.randint(100,150)\n",
    "rate_drop_lstm = 0.15 + 0.25*np.random.rand()\n",
    "rate_drop_dense = 0.15 + 0.25*np.random.rand()\n",
    "act = 'relu'\n",
    "re_weight = True\n",
    "STAMP = 'lstm_{}_{}_{:.2f}_{:.2f}'.format(num_lstm,num_dense,rate_drop_lstm,rate_drop_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取glove词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400000 word vectors of glove.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors.')\n",
    "embeddings_index = {}\n",
    "f = smart_open(EMBEDDING_FILE,encoding='utf-8',mode='r')\n",
    "for line in f.readlines():\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:],dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found {} word vectors of glove.'.format(len(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function 'text_to_wordlist' is from \n",
    "\n",
    "[https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # 取出停顿词，提取词干\n",
    "    \n",
    "    # 转换小写后分词\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # 去除停顿词\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    # 提取词干\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # 返回最终处理结果\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 这里只做选取10%的训练数据和5%的测试数据作为演示，即便如此，计算量也是非常大的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404290 pairs texts in train.csv\n",
      "Found 3563475 pairs texts in test.csv\n",
      "Choose 40429 samples in the train set\n",
      "Choose 178174 samples in the test set\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test = pd.read_csv(TEST_DATA_FILE)\n",
    "print(\"Found {} pairs texts in train.csv\".format(len(train)))\n",
    "print('Found {} pairs texts in test.csv'.format(len(test)))\n",
    "train = train.sample(frac=0.1)\n",
    "test = test.sample(frac=0.05)\n",
    "print('Choose {} samples in the train set'.format(len(train)))\n",
    "print('Choose {} samples in the test set'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[['question1','question2']] = train[['question1','question2']].astype(str)\n",
    "test[['question1','question2']] = test[['question1','question2']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost 29.37 sec for preprocessing train set.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# pandas会自动将read_csv中字符串中的数值转换为float或int，因此，需要强制声明为str\n",
    "train['texts_1'] = train['question1'].apply(lambda x: text_to_wordlist(text=str(x),remove_stopwords=True,stem_words=True))\n",
    "train['texts_2'] = train['question2'].apply(lambda x: text_to_wordlist(text=str(x),remove_stopwords=True,stem_words=True))\n",
    "print('Cost {:.2f} sec for preprocessing train set.'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost 136.44 sec for preprocessing test set.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "test['texts_1'] = test['question1'].apply(lambda x: text_to_wordlist(text=str(x),remove_stopwords=True,stem_words=True))\n",
    "test['texts_2'] = test['question2'].apply(lambda x: text_to_wordlist(text=str(x),remove_stopwords=True,stem_words=True))\n",
    "print('Cost {:.2f} sec for preprocessing test set.'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用keras中text的处理函数，进行分词，输入是text句子，通过对所有的的句子进行汇总，得到一个分词词典，词典的大小为MAX_NB_WORDS，如果真实字典比这个要大，那么根据单词词频排序，选择排名前MAX_NB_WORDS的单词，输出是句子中的单词在分词词典的id索引。\n",
    "\n",
    "fit_on_texts需要提供数据集中所有句子的总合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train['texts_1'].values.tolist()+train['texts_2'].values.tolist()+test['texts_1'].values.tolist()+test['texts_2'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_1 = tokenizer.texts_to_sequences(train['texts_1'].values)\n",
    "sequences_2 = tokenizer.texts_to_sequences(train['texts_2'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences_1 = tokenizer.texts_to_sequences(test['texts_1'].values)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test['texts_2'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "texts_to_sequences的输出是每条句子中各个分词在之前的分词字典的索引id组成的list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[232,\n",
       "  922,\n",
       "  1214,\n",
       "  1155,\n",
       "  30798,\n",
       "  3100,\n",
       "  44,\n",
       "  25523,\n",
       "  3100,\n",
       "  68,\n",
       "  25524,\n",
       "  3100,\n",
       "  88,\n",
       "  30799],\n",
       " [16711],\n",
       " [345, 10249],\n",
       " [131, 25, 1187, 1922, 4733],\n",
       " [2, 759, 136, 5208, 62, 9344],\n",
       " [2, 6530, 2991, 8198, 780, 362],\n",
       " [41, 354, 162],\n",
       " [143, 1931, 4064],\n",
       " [124, 23, 254, 135, 2388, 3],\n",
       " [51, 34, 20]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "113824    idea, prove (no computer) 253*sqrt(2) +874*sqr...\n",
       "73464                                                babar?\n",
       "325387                                      popular majors?\n",
       "262015                           busi start five ten lakhs?\n",
       "4739               get twitter give suspend account handle?\n",
       "70007                   get flexibl hors fenc solut sydney?\n",
       "333457                                   take stock market?\n",
       "296092                              human depend computers?\n",
       "218276                  earn money music video album india?\n",
       "203259                                   happen think much?\n",
       "Name: texts_1, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(sequences_1[:10])\n",
    "display(train.iloc[:10,]['texts_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65908 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found {} unique tokens.'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们sample出来的数据中，总共有50665个唯一的分词，这里面有些是会在glove的词典中的，有些是不在的，有些是因为拼写错误，所以不在glove中的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_1 = pad_sequences(sequences=sequences_1,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences=sequences_2,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(train['is_duplicate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (40429, 50)\n",
      "Shape of label tensor: (40429,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor: {}'.format(data_1.shape))\n",
    "print('Shape of label tensor: {}'.format(labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = pad_sequences(sequences=test_sequences_1,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(sequences=test_sequences_2,maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test['test_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate leaky features\n",
    "统计特征：问题搜索热度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is there any idea, how to prove (no computer) ...</td>\n",
       "      <td>Is there any idea, how to prove (no computer) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who was Babar?</td>\n",
       "      <td>Do you know Babar?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the most popular majors?</td>\n",
       "      <td>What is the most popular major in America?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which business can I start with five and ten l...</td>\n",
       "      <td>Which business can I start with $5000?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can I get Twitter to give me a suspended a...</td>\n",
       "      <td>Why was the Twitter account @Bill_Nye_tho susp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0  Is there any idea, how to prove (no computer) ...   \n",
       "1                                     Who was Babar?   \n",
       "2                  What are the most popular majors?   \n",
       "3  Which business can I start with five and ten l...   \n",
       "4  How can I get Twitter to give me a suspended a...   \n",
       "\n",
       "                                           question2  \n",
       "0  Is there any idea, how to prove (no computer) ...  \n",
       "1                                 Do you know Babar?  \n",
       "2         What is the most popular major in America?  \n",
       "3             Which business can I start with $5000?  \n",
       "4  Why was the Twitter account @Bill_Nye_tho susp...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ques = pd.concat([train[['question1','question2']],test[['question1','question2']]],axis=0,ignore_index=True)\n",
    "display(ques.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**q_dict**中记录的是一个广义`dic`，`key`为问题，对应的`value`为与`key`匹配的问题集set（去重）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "    q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "    q_dict[ques.question2[i]].add(ques.question1[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是计算一个问题的热度的函数，`q1_freq`计算了与`question1`匹配的问题有多少个，`q2_freq`计算了与`question2`匹配的问题有多少个，`q1_q2_intersect`计算了与`question1`和`question2`匹配问题字典的交集有多少个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_freq(row):\n",
    "    return(len(q_dict[row['question1']]))\n",
    "    \n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row['question2']]))\n",
    "    \n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['q1_q2_intersect'] = train.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "train['q1_freq'] = train.apply(q1_freq, axis=1, raw=True)\n",
    "train['q2_freq'] = train.apply(q2_freq, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['q1_q2_intersect'] = test.apply(q1_q2_intersect, axis=1)\n",
    "test['q1_freq'] = test.apply(q1_freq, axis=1)\n",
    "test['q2_freq'] = test.apply(q2_freq, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_q2_intersect = train.groupby('q1_q2_intersect')['is_duplicate'].mean().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = train[['q1_q2_intersect','q1_freq','q2_freq']]\n",
    "test_leaks = test[['q1_q2_intersect','q1_freq','q2_freq']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "scal = StandardScaler()\n",
    "scal.fit(np.concatenate((leaks,test_leaks),axis=0))\n",
    "leaks = scal.transform(leaks)\n",
    "test_leaks = scal.transform(test_leaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 提取Glove词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix.\n",
      "Null word embeddings: 24365\n"
     ]
    }
   ],
   "source": [
    "print('Preparing embedding matrix.')\n",
    "nb_words = min(MAX_NB_WORDS,len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((nb_words,EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('Null word embeddings: {}'.format(np.sum(np.sum(embedding_matrix,axis=1)==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_valid = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.concatenate((data_1[idx_train],data_2[idx_train]),axis=0)\n",
    "data_2_train = np.concatenate((data_2[idx_train],data_1[idx_train]),axis=0)\n",
    "leaks_train = np.concatenate((leaks[idx_train],leaks[idx_train]),axis=0)\n",
    "labels_train = np.concatenate((labels[idx_train],labels[idx_train]))\n",
    "\n",
    "data_1_valid = np.concatenate((data_1[idx_valid],data_2[idx_valid]),axis=0)\n",
    "data_2_valid = np.concatenate((data_2[idx_valid],data_1[idx_valid]),axis=0)\n",
    "leaks_valid = np.concatenate((leaks[idx_valid],leaks[idx_valid]),axis=0)\n",
    "labels_valid = np.concatenate((labels[idx_valid],labels[idx_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_val = np.ones(len(labels_valid))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_valid==0] = 1.309028344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "模型包括两个输入和一个输出，输入为两个需要对比的text，经embedding层后，各自经过一个双向LSTM，两个的输出结果与leaky feature经过dense后的输出结果，拼接成一个新的特征向量，输入到最终的分类全连接层，全连接层的输出为两个句子是否匹配的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(input_dim=nb_words,\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            output_dim=EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_layer = Bidirectional(LSTM(units=num_lstm,\n",
    "                                dropout=rate_drop_lstm,\n",
    "                                recurrent_dropout=rate_drop_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32',name='sequences_input1')\n",
    "embedded_sequnces_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequnces_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,),dtype='int32',name='sequences_input2')\n",
    "embedded_sequnces_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequnces_2)\n",
    "\n",
    "leaks_input = Input(shape=(leaks.shape[1],),dtype='float32',name='leaks_input')\n",
    "leaks_dense = Dense(units=int(num_dense/2),activation=act)(leaks_input)\n",
    "\n",
    "merged = concatenate([x1,y1,leaks_dense])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "merged = Dense(num_dense,activation=act)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "\n",
    "preds = Dense(units=1,activation='sigmoid',name='prediction')(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过损失权重平衡样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if re_weight:\n",
    "    class_weight = {0:1.3090,\n",
    "                    1:0.4720}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "sequences_input1 (InputLayer)   (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequences_input2 (InputLayer)   (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 50)       3295450     sequences_input1[0][0]           \n",
      "                                                                 sequences_input2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "leaks_input (InputLayer)        (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 544)          702848      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 74)           296         leaks_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1162)         0           bidirectional_1[0][0]            \n",
      "                                                                 bidirectional_1[1][0]            \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1162)         4648        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1162)         0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 149)          173287      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 149)          596         dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 149)          0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            150         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,177,275\n",
      "Trainable params: 879,203\n",
      "Non-trainable params: 3,298,072\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[sequence_1_input,sequence_2_input,leaks_input],outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['acc'])\n",
    "print(model.summary())\n",
    "#plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm_272_149_0.22_0.25\n"
     ]
    }
   ],
   "source": [
    "print(STAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',patience=3,verbose=1)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(filepath=bst_model_path,\n",
    "                                   save_best_only=True,\n",
    "                                   save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64686, 50)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 64686 samples, validate on 16172 samples\n",
      "Epoch 1/20\n",
      "24704/64686 [==========>...................] - ETA: 3:40 - loss: 0.4844 - acc: 0.6771"
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=[data_1_train,data_2_train,leaks_train],\n",
    "                 y=labels_train,\n",
    "                 batch_size=128,\n",
    "                 epochs=20,\n",
    "                 shuffle=True,\n",
    "                 validation_data=([data_1_valid,data_2_valid,leaks_valid],\n",
    "                                  labels_valid,weight_val),\n",
    "                 class_weight=class_weight,\n",
    "                 callbacks=[early_stopping,model_checkpoint],\n",
    "                 verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "笔记本的GPU计算性能有限，这里就不继续跑了。在Kaggle官方提供的计算平台上，经过30个epoch，最终loss大致在0.19，acc为0.9。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
